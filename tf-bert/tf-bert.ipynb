{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "858a2b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebaf934fd1294f22ad57606730430098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/536M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "# Pretrained model\n",
    "CHECKPOINT = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "pretrained_model = TFAutoModelForSequenceClassification.from_pretrained(CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce93a898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT fine-tuning with batched Dataset\n",
    "# texts と labels から BERT 入力用の batch 化された Dataset を生成するミニマルな処理\n",
    "\n",
    "# tokenize する\n",
    "def tokenize(texts):\n",
    "    '''BERT用tokenizerでtokenizeする\n",
    "    \n",
    "    Args:\n",
    "        texts (list<str>): モデルに入力されるテキストのリスト\n",
    "        \n",
    "    Returns:\n",
    "        dict: BERT用トークナイザの戻り値(後続処理のためにpaddingしたもの)\n",
    "        \n",
    "    '''\n",
    "    return tokenizer(texts,\n",
    "                     max_length=6,\n",
    "                     truncation=True,\n",
    "                     padding='max_length',\n",
    "                     add_special_tokens=True,\n",
    "                     return_tensors='tf')\n",
    "\n",
    "def get_dataset(bert_tokenized_texts, labels):\n",
    "    '''BERT用トークナイザでトークナイズされたテキストとラベル列からシンプルなDatasetを作る.\n",
    "    dataset の列の並び順は任意だが, 次のステップの処理はこの並び順に依存する. \n",
    "    本コードでは input_ids, attention_mask, labels の順とする.\n",
    "    \n",
    "    Args:\n",
    "        bert_tokenized_texts (dict): BERT用トークナイザの戻り値(後続処理のために要padding)\n",
    "        labels (list): ラベル列\n",
    "        \n",
    "    Returns:\n",
    "        tensorflow.data.Dataset: input_ids, attention_mask, labels 列を持つシンプルなデータセット\n",
    "        \n",
    "    '''\n",
    "    return tf.data.Dataset.from_tensor_slices((bert_tokenized_texts['input_ids'],\n",
    "                                               bert_tokenized_texts['attention_mask'],\n",
    "                                               labels))\n",
    "\n",
    "def get_bert_compatible_dataset(dataset):\n",
    "    '''dataset を huggingface.transformers の BERT に入力可能な形式に変換する.\n",
    "    \n",
    "    Args:\n",
    "        dataset (tensorflow.data.Dataset): input_ids, attention_mask, labels 列を\n",
    "                                           持つシンプルなデータセット\n",
    "    Returns:\n",
    "        dataset (tensorflow.data.MapDataset): 各 instance が BERT の入力形式に変換されたデータセット\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def _get_bert_compatible_instance(input_ids, attention_mask, labels):\n",
    "        '''Dataset.map() 用の関数. Dataset の各 instance を BERT の入力形式に変換する.\n",
    "        引数の並び順は呼び出し元Datasetの列の並び順に依存している(前のステップで定められている).\n",
    "\n",
    "        Args:\n",
    "            input_ids (list): \n",
    "            attention_mask (list):\n",
    "            labels (list):\n",
    "\n",
    "        Returns:\n",
    "            tuple: ({'input_ids': input_ids, 'attention_mask': attention_mask}, label) のタプル\n",
    "\n",
    "        '''\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        }, labels\n",
    "    return dataset.map(_get_bert_compatible_instance)\n",
    "\n",
    "# batch 化する\n",
    "def get_batched_dataset(dataset, batch_size):\n",
    "    '''バッチ化されたデータセットを作成する\n",
    "    \n",
    "    Args:\n",
    "        dataset (tensorflow.data.Dataset): 任意のデータセット\n",
    "        batch_size (int): バッチサイズ\n",
    "    Returns:\n",
    "        tensorflow.data.BatchDataset: バッチ化されたデータセット\n",
    "    \n",
    "    '''\n",
    "    return dataset.batch(batch_size=batch_size)\n",
    "\n",
    "\n",
    "# 処理\n",
    "def get_bert_batch_dataset(texts, labels, batch_size=32):\n",
    "    '''テキスト列とラベル列からバッチ化されたBERT入力用のデータセットを作成する\n",
    "    \n",
    "    Args:\n",
    "        texts (list): BERT の fine-tuning に用いるテキスト列\n",
    "        labels (list): BERT の fine-tuning に用いるラベル列\n",
    "        \n",
    "    Returns:\n",
    "        tensorflow.data.BatchDataset: バッチ化されたBERT入力用のデータセット\n",
    "    \n",
    "    '''\n",
    "    bert_tokenized_texts = tokenize(texts)\n",
    "    dataset = get_dataset(bert_tokenized_texts, labels)\n",
    "    dataset = get_bert_compatible_dataset(dataset)\n",
    "#     dataset = get_batched_dataset(dataset, batch_size=batch_size)\n",
    "    return dataset\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "# **\n",
    "# Create Dataset (Does the statement contains \"cat\" ?)\n",
    "# *\n",
    "texts_train = ['I like cat',\n",
    "               'I do not like cat',\n",
    "               'I like dog',\n",
    "               'I do not like dog']\n",
    "labels_train = [1,\n",
    "                1,\n",
    "                0,\n",
    "                0]\n",
    "texts_valid = ['I love cat',\n",
    "               'I am cat',\n",
    "               'I love dog',\n",
    "               'I am dog']\n",
    "labels_valid = [1,\n",
    "                1,\n",
    "                0,\n",
    "                0]\n",
    "texts_test = ['cat walked away from me',\n",
    "              'I miss my dog']\n",
    "\n",
    "ds_train = get_bert_batch_dataset(texts_train, labels_train, batch_size=batch_size)\n",
    "ds_valid = get_bert_batch_dataset(texts_valid, labels_valid, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3c014a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **\n",
    "# Construct model\n",
    "# *\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(pretrained_model)\n",
    "model.add(tf.keras.layers.Dense(2))\n",
    "\n",
    "# BERT は最終層のみ Fine tuning\n",
    "model.layers[0].layers[0].trainable = False\n",
    "model.layers[0].layers[1].trainable = False\n",
    "model.layers[0].layers[2].trainable = True\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='accuracy',\n",
    "              metrics='accuracy')\n",
    "\n",
    "# **\n",
    "# Train\n",
    "# *\n",
    "# Callbacks\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=4, verbose=0)\n",
    "\n",
    "# # # Training\n",
    "# history = model.fit(ds_train,\n",
    "#                     epochs=2,\n",
    "#                     batch_size=batch_size,\n",
    "# #                     validation_data=ds_valid,\n",
    "#                     callbacks=[early_stopping])\n",
    "\n",
    "# # Learning curve\n",
    "# pd.DataFrame({'train': history.history['loss'],\n",
    "#               'valid': history.history['val_loss']}).plot()\n",
    "\n",
    "# **\n",
    "# Predict\n",
    "# *\n",
    "# for X, _ in ds_train:\n",
    "#     print(model.predict_on_batch(X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
