{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72a170f7",
   "metadata": {},
   "source": [
    "# tf-bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd1181f",
   "metadata": {},
   "source": [
    "## 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a6955f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "# **\n",
    "# Create Dataset (Does the statement contains \"cat\" ?)\n",
    "# *\n",
    "texts_train = ['I like cat',\n",
    "               'I do not like cat',\n",
    "               'You are like a cat',\n",
    "               'A cat lover never gives up',\n",
    "               'He walks like a cat',\n",
    "               'My favorite animal is cat',\n",
    "               'I like dog',\n",
    "               'I do not like dog',\n",
    "               'You are like a dog',\n",
    "               'Dog lovers always gives up',\n",
    "               'She walks like a dog',\n",
    "               'My favorite animal is dog',]\n",
    "labels_train = [1,\n",
    "                1,\n",
    "                1,\n",
    "                1,\n",
    "                1,\n",
    "                1,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0]\n",
    "texts_valid = ['I love cat',\n",
    "               'I wish I had a cat',\n",
    "               'I am cat',\n",
    "               'I love dog',\n",
    "               'I wish I had a dog',\n",
    "               'I am dog',]\n",
    "labels_valid = [1,\n",
    "                1,\n",
    "                1,\n",
    "                0,\n",
    "                0,\n",
    "                0]\n",
    "texts_test = ['cat walked away from me',\n",
    "              'I miss my dog']\n",
    "labels_test = [1,\n",
    "               0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3063308a",
   "metadata": {},
   "source": [
    "## 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3671248a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-05 23:58:00.668471: I tensorflow/core/util/util.cc:175] Experimental oneDNN custom operations are on. If you experience issues, please turn them off by setting the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from transformers import AutoTokenizer, TFBertForSequenceClassification\n",
    "CHECKPOINT = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdec25b5",
   "metadata": {},
   "source": [
    "Notebook の構成\n",
    "* セルごとに Stand-alone で動作するように記述\n",
    "\n",
    "仕様\n",
    "* 出力はラベルではなく logits とする(loss の算出のため)\n",
    "\n",
    "TODO\n",
    "\n",
    "* 学習パイプラインのバリデーション（重み更新できてるのか、学習前後で比較し確認）\n",
    "* データ、数件では無理があるのでちゃんとしたものに変更\n",
    "* input・output を見直し End-to-end のネットワークに変更\n",
    "  * input: Tensor\\[str\\] を受け取り tokenize・encoding する層に変更（WARNING 解消のため; ベストプラクティスなのかは要確認）\n",
    "  * output: bert の logits を受け取り dense 等任意の層で受け取る実装に変更（テーブルデータとの concat 実装等、拡張する際必要）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ee961",
   "metadata": {},
   "source": [
    "### 2.1 素のモデル"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa259cb",
   "metadata": {},
   "source": [
    "学習できている様子を示すためにはちゃんとしたデータの用意・タスクの設定が必要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af2c777b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 8s 1s/step - loss: 1.5167 - accuracy: 0.5000 - val_loss: 0.9288 - val_accuracy: 0.5000\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 1s 279ms/step - loss: 1.9289 - accuracy: 0.5000 - val_loss: 0.9288 - val_accuracy: 0.5000\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 1s 284ms/step - loss: 0.9280 - accuracy: 0.5000 - val_loss: 0.9288 - val_accuracy: 0.5000\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 1s 277ms/step - loss: 1.5158 - accuracy: 0.5000 - val_loss: 0.9288 - val_accuracy: 0.5000\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 1s 277ms/step - loss: 2.6641 - accuracy: 0.5000 - val_loss: 0.9288 - val_accuracy: 0.5000\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 1s 295ms/step - loss: 1.4478 - accuracy: 0.5000 - val_loss: 0.9288 - val_accuracy: 0.5000\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 1s 279ms/step - loss: 0.9496 - accuracy: 0.5000 - val_loss: 0.9288 - val_accuracy: 0.5000\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 1s 281ms/step - loss: 1.4446 - accuracy: 0.5000 - val_loss: 0.9288 - val_accuracy: 0.5000\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 1s 276ms/step - loss: 1.0369 - accuracy: 0.5000 - val_loss: 0.9288 - val_accuracy: 0.5000\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 1s 282ms/step - loss: 1.6836 - accuracy: 0.5000 - val_loss: 0.9288 - val_accuracy: 0.5000\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 1s 280ms/step - loss: 1.9766 - accuracy: 0.5000 - val_loss: 0.9288 - val_accuracy: 0.5000\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 1s 280ms/step - loss: 1.4513 - accuracy: 0.5000 - val_loss: 0.9288 - val_accuracy: 0.5000\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 1s 273ms/step - loss: 1.4781 - accuracy: 0.5000 - val_loss: 0.9288 - val_accuracy: 0.5000\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 1s 289ms/step - loss: 0.9280 - accuracy: 0.5000 - val_loss: 0.9288 - val_accuracy: 0.5000\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 1s 282ms/step - loss: 2.0728 - accuracy: 0.5000 - val_loss: 0.9288 - val_accuracy: 0.5000\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 1s 283ms/step - loss: 1.4960 - accuracy: 0.5000 - val_loss: 0.9288 - val_accuracy: 0.5000\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 1s 280ms/step - loss: 1.5511 - accuracy: 0.5000 - val_loss: 0.9288 - val_accuracy: 0.5000\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 1s 280ms/step - loss: 2.1368 - accuracy: 0.5000 - val_loss: 0.9288 - val_accuracy: 0.5000\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 1s 274ms/step - loss: 1.5155 - accuracy: 0.5000 - val_loss: 0.9288 - val_accuracy: 0.5000\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 1s 283ms/step - loss: 1.5522 - accuracy: 0.5000 - val_loss: 0.9288 - val_accuracy: 0.5000\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 1s 282ms/step - loss: 1.6474 - accuracy: 0.5000 - val_loss: 0.9288 - val_accuracy: 0.5000\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 1s 278ms/step - loss: 1.4962 - accuracy: 0.5000 - val_loss: 0.9288 - val_accuracy: 0.5000\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 1s 281ms/step - loss: 1.5297 - accuracy: 0.5000 - val_loss: 0.9288 - val_accuracy: 0.5000\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 1s 277ms/step - loss: 1.5745 - accuracy: 0.5000 - val_loss: 0.9288 - val_accuracy: 0.5000\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 1s 276ms/step - loss: 0.8532 - accuracy: 0.5000 - val_loss: 0.9288 - val_accuracy: 0.5000\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 1s 276ms/step - loss: 2.2003 - accuracy: 0.5000 - val_loss: 0.9288 - val_accuracy: 0.5000\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 1s 282ms/step - loss: 2.0068 - accuracy: 0.5000 - val_loss: 0.9288 - val_accuracy: 0.5000\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 1s 274ms/step - loss: 1.7208 - accuracy: 0.5000 - val_loss: 0.9288 - val_accuracy: 0.5000\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 1s 274ms/step - loss: 1.8265 - accuracy: 0.5000 - val_loss: 0.9288 - val_accuracy: 0.5000\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 1s 277ms/step - loss: 1.6497 - accuracy: 0.5000 - val_loss: 0.9288 - val_accuracy: 0.5000\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 1s 280ms/step - loss: 2.0269 - accuracy: 0.5000 - val_loss: 0.9288 - val_accuracy: 0.5000\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "def get_dataset(tokenizer, texts, labels=None, batch_size=10):\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True)\n",
    "    if labels:\n",
    "        return tf.data.Dataset.from_tensor_slices((dict(encodings), labels)).batch(batch_size)\n",
    "    else:\n",
    "        return tf.data.Dataset.from_tensor_slices((dict(encodings))).batch(batch_size)\n",
    "        \n",
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "ds_train = get_dataset(tokenizer, texts_train, labels=labels_train)\n",
    "ds_valid = get_dataset(tokenizer, texts_valid, labels=labels_valid)\n",
    "ds_test  = get_dataset(tokenizer, texts_test)\n",
    "\n",
    "# Model\n",
    "pretrained_model = TFBertForSequenceClassification.from_pretrained(CHECKPOINT)\n",
    "pretrained_model.trainable = False                       # 全層重みを一時的に freeze\n",
    "pretrained_model.bert.encoder.layer[-1].trainable = True # BERT 最終層は trainable\n",
    "pretrained_model.layers[-1].trainable = True             # 全結合層は trainable\n",
    "pretrained_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00000001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics='accuracy')\n",
    "\n",
    "# Train\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=30, verbose=0)\n",
    "history = pretrained_model.fit(ds_train,\n",
    "                               epochs=100,\n",
    "                               batch_size=batch_size,\n",
    "                               validation_data=ds_valid,\n",
    "                               callbacks=[early_stopping])\n",
    "\n",
    "# Predict label\n",
    "get_pred = lambda x: np.argmax(pretrained_model.predict(x)['logits'], axis=1)\n",
    "get_pred(ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0e0bc9",
   "metadata": {},
   "source": [
    "## 2.2 Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e424e4e",
   "metadata": {},
   "source": [
    "WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68b2d701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'input_ids': <tf.Tensor 'IteratorGetNext:1' shape=(None, 7) dtype=int32>, 'token_type_ids': <tf.Tensor 'IteratorGetNext:2' shape=(None, 7) dtype=int32>, 'attention_mask': <tf.Tensor 'IteratorGetNext:0' shape=(None, 7) dtype=int32>}. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/home/ubuntu/python3-venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/ubuntu/python3-venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/ubuntu/python3-venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/ubuntu/python3-venv/lib/python3.8/site-packages/keras/engine/training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/ubuntu/python3-venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"<string>\", line 3, in raise_from\n        \n\n    TypeError: Exception encountered when calling layer \"sequential_10\" \"                 f\"(type Sequential).\n    \n    Failed to convert 'TFSequenceClassifierOutput(loss=None, logits=TensorShape([None, 2]), hidden_states=None, attentions=None)' to a shape: ''logits''could not be converted to a dimension. A shape should either be single dimension (e.g. 10), or an iterable of dimensions (e.g. [1, 10, None]).\n    \n    Call arguments received by layer \"sequential_10\" \"                 f\"(type Sequential):\n      • inputs={'input_ids': 'tf.Tensor(shape=(None, 7), dtype=int32)', 'token_type_ids': 'tf.Tensor(shape=(None, 7), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(None, 7), dtype=int32)'}\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 25\u001b[0m\n\u001b[1;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m),\n\u001b[1;32m     20\u001b[0m               loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     21\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[1;32m     28\u001b[0m model\u001b[38;5;241m.\u001b[39mpredict(ds_test)\n",
      "File \u001b[0;32m~/python3-venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file5ltau_tq.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/home/ubuntu/python3-venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/ubuntu/python3-venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/ubuntu/python3-venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/ubuntu/python3-venv/lib/python3.8/site-packages/keras/engine/training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/ubuntu/python3-venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"<string>\", line 3, in raise_from\n        \n\n    TypeError: Exception encountered when calling layer \"sequential_10\" \"                 f\"(type Sequential).\n    \n    Failed to convert 'TFSequenceClassifierOutput(loss=None, logits=TensorShape([None, 2]), hidden_states=None, attentions=None)' to a shape: ''logits''could not be converted to a dimension. A shape should either be single dimension (e.g. 10), or an iterable of dimensions (e.g. [1, 10, None]).\n    \n    Call arguments received by layer \"sequential_10\" \"                 f\"(type Sequential):\n      • inputs={'input_ids': 'tf.Tensor(shape=(None, 7), dtype=int32)', 'token_type_ids': 'tf.Tensor(shape=(None, 7), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(None, 7), dtype=int32)'}\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "train_encodings = tokenizer(texts_train, truncation=True, padding=True, return_tensors='tf')\n",
    "ds_train = tf.data.Dataset \\\n",
    "                        .from_tensor_slices((dict(train_encodings), labels_train)) \\\n",
    "                        .batch(2)\n",
    "test_encodings = tokenizer(texts_test, truncation=True, padding=True)\n",
    "ds_test = tf.data.Dataset \\\n",
    "                        .from_tensor_slices((dict(test_encodings))) \\\n",
    "                        .batch(2)\n",
    "### TODO: Data validation should be implemented\n",
    "\n",
    "# Model\n",
    "CHECKPOINT = 'bert-base-uncased'\n",
    "pretrained_model = TFBertForSequenceClassification.from_pretrained(CHECKPOINT)\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(pretrained_model)\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics='accuracy')\n",
    "\n",
    "\n",
    "# Train\n",
    "model.fit(ds_train, epochs=2)\n",
    "\n",
    "# Predict\n",
    "model.predict(ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f96ec61",
   "metadata": {},
   "source": [
    "## Bak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accf06f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(pretrained_model)\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "# BERT は最終層のみ Fine tuning\n",
    "model.layers[0].layers[0].trainable = False\n",
    "model.layers[0].layers[1].trainable = False\n",
    "model.layers[0].layers[2].trainable = True\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics='accuracy')\n",
    "# Train\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=4, verbose=0)\n",
    "history = model.fit(ds_train,\n",
    "                    epochs=2,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=ds_valid,\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "# # Learning curve\n",
    "# pd.DataFrame({'train': history.history['loss'],\n",
    "#               'valid': history.history['val_loss']}).plot()\n",
    "\n",
    "# **\n",
    "# Predict\n",
    "# *\n",
    "# for X, _ in ds_train:\n",
    "#     print(model.predict_on_batch(X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
