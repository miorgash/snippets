{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72a170f7",
   "metadata": {},
   "source": [
    "# tf-bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd1181f",
   "metadata": {},
   "source": [
    "## 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a6955f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "# **\n",
    "# Create Dataset (Does the statement contains \"cat\" ?)\n",
    "# *\n",
    "texts_train = ['I like cat',\n",
    "               'I do not like cat',\n",
    "               'I like dog',\n",
    "               'I do not like dog']\n",
    "labels_train = [1,\n",
    "                1,\n",
    "                0,\n",
    "                0]\n",
    "texts_valid = ['I love cat',\n",
    "               'I am cat',\n",
    "               'I love dog',\n",
    "               'I am dog']\n",
    "labels_valid = [1,\n",
    "                1,\n",
    "                0,\n",
    "                0]\n",
    "texts_test = ['cat walked away from me',\n",
    "              'I miss my dog']\n",
    "labels_test = [1,\n",
    "               0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3063308a",
   "metadata": {},
   "source": [
    "## 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c77fe1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, TFBertForSequenceClassification\n",
    "CHECKPOINT = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745a9717",
   "metadata": {},
   "source": [
    "セルごとに Stand-alone で動作するように記述"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762458ea",
   "metadata": {},
   "source": [
    "仕様\n",
    "* 出力はラベルではなく logits とする(loss の算出のため)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42d8cc3",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "* 学習パイプラインのバリデーション（重み更新できてるのか、学習前後で比較し確認）\n",
    "* データ、数件では無理があるのでちゃんとしたものに変更\n",
    "* input・output を見直し End-to-end のネットワークに変更\n",
    "  * input: Tensor\\[str\\] を受け取り tokenize・encoding する層に変更（WARNING 解消のため; ベストプラクティスなのかは要確認）\n",
    "  * output: bert の logits を受け取り dense 等任意の層で受け取る実装に変更（テーブルデータとの concat 実装等、拡張する際必要）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ee961",
   "metadata": {},
   "source": [
    "### 2.1 素のモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "af2c777b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 6s 146ms/step - loss: 6.1093 - accuracy: 0.5000\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 7.7125 - accuracy: 0.5000\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 7.7125 - accuracy: 0.5000\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 7.7125 - accuracy: 0.5000\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 6.0629 - accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 4.9080 - accuracy: 0.2500\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 6.2857 - accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 6.0052 - accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 4.8497 - accuracy: 0.5000\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 7.7185 - accuracy: 0.7500\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 7.7203 - accuracy: 0.7500\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 7.7125 - accuracy: 0.7500\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 7.7125 - accuracy: 0.5000\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 7.7125 - accuracy: 0.2500\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 7.7125 - accuracy: 0.5000\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 7.7125 - accuracy: 0.5000\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 7.7218 - accuracy: 0.7500\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 7.7125 - accuracy: 0.7500\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 7.7125 - accuracy: 0.5000\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 7.7158 - accuracy: 0.2500\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 7.7199 - accuracy: 0.2500\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 7.7358 - accuracy: 0.5000\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 6.0746 - accuracy: 0.5000\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 6.2382 - accuracy: 0.5000\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 7.7125 - accuracy: 0.5000\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 7.7125 - accuracy: 0.5000\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 6.1876 - accuracy: 0.7500\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 7.7125 - accuracy: 0.5000\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 6.1924 - accuracy: 0.2500\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 6.1219 - accuracy: 0.5000\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 6.3068 - accuracy: 0.7500\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 6.3644 - accuracy: 0.2500\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 4.9087 - accuracy: 0.2500\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 7.7179 - accuracy: 0.7500\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 6.2241 - accuracy: 0.2500\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 6.2265 - accuracy: 0.5000\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 6.0320 - accuracy: 0.7500\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 4.5948 - accuracy: 0.7500\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 7.7311 - accuracy: 0.5000\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 5.3163 - accuracy: 0.5000\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 7.7125 - accuracy: 0.5000\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 7.7125 - accuracy: 0.7500\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 7.7125 - accuracy: 0.2500\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 6.1089 - accuracy: 0.5000\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 6.1506 - accuracy: 0.5000\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 6.3694 - accuracy: 0.5000\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 7.7166 - accuracy: 0.2500\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 7.7125 - accuracy: 0.7500\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 6.2897 - accuracy: 0.5000\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 7.7241 - accuracy: 0.5000\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 7.7214 - accuracy: 0.5000\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 7.7172 - accuracy: 0.5000\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 6.2264 - accuracy: 0.5000\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 6.1004 - accuracy: 0.7500\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 6.2406 - accuracy: 0.5000\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 6.0841 - accuracy: 0.5000\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 7.7125 - accuracy: 0.5000\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 7.7168 - accuracy: 0.2500\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 7.7125 - accuracy: 0.5000\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 7.7125 - accuracy: 0.5000\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 7.7125 - accuracy: 0.7500\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 7.7247 - accuracy: 0.5000\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 7.7125 - accuracy: 0.5000\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 7.7133 - accuracy: 0.0000e+00\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 177ms/step - loss: 7.7303 - accuracy: 0.5000\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 6.2794 - accuracy: 0.5000\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 6.1962 - accuracy: 0.7500\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 7.7170 - accuracy: 0.5000\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 7.7125 - accuracy: 0.5000\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 7.7401 - accuracy: 0.2500\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 6.3790 - accuracy: 0.7500\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 7.7125 - accuracy: 0.2500\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 7.7156 - accuracy: 0.5000\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 6.0690 - accuracy: 0.5000\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 7.7125 - accuracy: 0.5000\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 7.7125 - accuracy: 0.5000\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 7.7125 - accuracy: 0.0000e+00\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 6.1136 - accuracy: 0.2500\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 6.8514 - accuracy: 0.7500\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 7.7125 - accuracy: 0.2500\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 7.7125 - accuracy: 0.5000\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 140ms/step - loss: 7.7125 - accuracy: 0.5000\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 7.7125 - accuracy: 0.7500\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 7.7176 - accuracy: 0.2500\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 6.0811 - accuracy: 0.5000\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 7.7258 - accuracy: 0.5000\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 6.1789 - accuracy: 0.5000\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 4.8022 - accuracy: 0.7500\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 4.9943 - accuracy: 0.5000\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 6.2059 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 6.1162 - accuracy: 0.5000\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 6.4224 - accuracy: 0.5000\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 7.7163 - accuracy: 0.5000\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 7.7162 - accuracy: 0.5000\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 7.7214 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 7.7226 - accuracy: 0.5000\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 7.7143 - accuracy: 0.2500\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 7.7125 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 6.3058 - accuracy: 0.7500\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 6.1341 - accuracy: 0.5000\n",
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "train_encodings = tokenizer(texts_train, truncation=True, padding=True)\n",
    "ds_train = tf.data.Dataset \\\n",
    "                        .from_tensor_slices((dict(train_encodings), labels_train)) \\\n",
    "                        .batch(2)\n",
    "test_encodings = tokenizer(texts_test, truncation=True, padding=True)\n",
    "ds_test = tf.data.Dataset \\\n",
    "                        .from_tensor_slices((dict(test_encodings))) \\\n",
    "                        .batch(2)\n",
    "\n",
    "# Model\n",
    "pretrained_model = TFBertForSequenceClassification.from_pretrained(CHECKPOINT)\n",
    "# freeze\n",
    "pretrained_model.trainable = False\n",
    "# bert 最終層を trainable に変更\n",
    "pretrained_model.bert.encoder.layer[-1].trainable = True\n",
    "# 全結合層を trainable に変更\n",
    "pretrained_model.layers[-1].trainable = True\n",
    "pretrained_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=-0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics='accuracy')\n",
    "\n",
    "# Train\n",
    "pretrained_model.fit(ds_train, epochs=100)\n",
    "\n",
    "# Predict label\n",
    "get_pred = lambda x: np.argmax(pretrained_model.predict(x)['logits'], axis=1)\n",
    "get_pred(ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0e0bc9",
   "metadata": {},
   "source": [
    "## 2.2 Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629192ac",
   "metadata": {},
   "source": [
    "WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "712b6134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'input_ids': <tf.Tensor 'IteratorGetNext:1' shape=(None, 7) dtype=int32>, 'token_type_ids': <tf.Tensor 'IteratorGetNext:2' shape=(None, 7) dtype=int32>, 'attention_mask': <tf.Tensor 'IteratorGetNext:0' shape=(None, 7) dtype=int32>}. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/home/ubuntu/python3-venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/ubuntu/python3-venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/ubuntu/python3-venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/ubuntu/python3-venv/lib/python3.8/site-packages/keras/engine/training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/ubuntu/python3-venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"<string>\", line 3, in raise_from\n        \n\n    TypeError: Exception encountered when calling layer \"sequential_10\" \"                 f\"(type Sequential).\n    \n    Failed to convert 'TFSequenceClassifierOutput(loss=None, logits=TensorShape([None, 2]), hidden_states=None, attentions=None)' to a shape: ''logits''could not be converted to a dimension. A shape should either be single dimension (e.g. 10), or an iterable of dimensions (e.g. [1, 10, None]).\n    \n    Call arguments received by layer \"sequential_10\" \"                 f\"(type Sequential):\n      • inputs={'input_ids': 'tf.Tensor(shape=(None, 7), dtype=int32)', 'token_type_ids': 'tf.Tensor(shape=(None, 7), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(None, 7), dtype=int32)'}\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 25\u001b[0m\n\u001b[1;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m),\n\u001b[1;32m     20\u001b[0m               loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     21\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[1;32m     28\u001b[0m model\u001b[38;5;241m.\u001b[39mpredict(ds_test)\n",
      "File \u001b[0;32m~/python3-venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file5ltau_tq.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/home/ubuntu/python3-venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/ubuntu/python3-venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/ubuntu/python3-venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/ubuntu/python3-venv/lib/python3.8/site-packages/keras/engine/training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/ubuntu/python3-venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"<string>\", line 3, in raise_from\n        \n\n    TypeError: Exception encountered when calling layer \"sequential_10\" \"                 f\"(type Sequential).\n    \n    Failed to convert 'TFSequenceClassifierOutput(loss=None, logits=TensorShape([None, 2]), hidden_states=None, attentions=None)' to a shape: ''logits''could not be converted to a dimension. A shape should either be single dimension (e.g. 10), or an iterable of dimensions (e.g. [1, 10, None]).\n    \n    Call arguments received by layer \"sequential_10\" \"                 f\"(type Sequential):\n      • inputs={'input_ids': 'tf.Tensor(shape=(None, 7), dtype=int32)', 'token_type_ids': 'tf.Tensor(shape=(None, 7), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(None, 7), dtype=int32)'}\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "train_encodings = tokenizer(texts_train, truncation=True, padding=True, return_tensors='tf')\n",
    "ds_train = tf.data.Dataset \\\n",
    "                        .from_tensor_slices((dict(train_encodings), labels_train)) \\\n",
    "                        .batch(2)\n",
    "test_encodings = tokenizer(texts_test, truncation=True, padding=True)\n",
    "ds_test = tf.data.Dataset \\\n",
    "                        .from_tensor_slices((dict(test_encodings))) \\\n",
    "                        .batch(2)\n",
    "### TODO: Data validation should be implemented\n",
    "\n",
    "# Model\n",
    "CHECKPOINT = 'bert-base-uncased'\n",
    "pretrained_model = TFBertForSequenceClassification.from_pretrained(CHECKPOINT)\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(pretrained_model)\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics='accuracy')\n",
    "\n",
    "\n",
    "# Train\n",
    "model.fit(ds_train, epochs=2)\n",
    "\n",
    "# Predict\n",
    "model.predict(ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c894d3e",
   "metadata": {},
   "source": [
    "```\n",
    "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: ...\n",
    "```\n",
    "\n",
    "Dataset の要素は tensor じゃないと遅くなってしまったりする?  \n",
    "vectorizer_layer ならぬ encoding_layer を用意するべき?(受け取るのは Tensor\\[str\\])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f131ea7",
   "metadata": {},
   "source": [
    "## Bak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c819a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(pretrained_model)\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "# BERT は最終層のみ Fine tuning\n",
    "model.layers[0].layers[0].trainable = False\n",
    "model.layers[0].layers[1].trainable = False\n",
    "model.layers[0].layers[2].trainable = True\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics='accuracy')\n",
    "# Train\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=4, verbose=0)\n",
    "history = model.fit(ds_train,\n",
    "                    epochs=2,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=ds_valid,\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "# # Learning curve\n",
    "# pd.DataFrame({'train': history.history['loss'],\n",
    "#               'valid': history.history['val_loss']}).plot()\n",
    "\n",
    "# **\n",
    "# Predict\n",
    "# *\n",
    "# for X, _ in ds_train:\n",
    "#     print(model.predict_on_batch(X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
